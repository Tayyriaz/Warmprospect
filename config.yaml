# Scraping Configuration
# This file contains default settings for website scraping and knowledge base building.
# You can override these values using environment variables in your .env file.

scraping:
  # Maximum crawling depth
  max_depth: 5
  
  # Maximum number of pages to scrape
  max_pages: 500
  
  # Maximum time to spend scraping (in seconds)
  # Increase if you have many pages or slow rate limits
  max_seconds: 1800  # 30 minutes (increased to handle large sites with rate limiting)
  
  # Maximum number of links to queue per page
  max_links_per_page: 30
  
  # Maximum queue size to prevent memory issues
  max_queue_size: 1000
  
  # SSL verification for website scraping
  # Set to false to disable SSL verification (useful for sites with self-signed certs)
  verify_ssl: false

  # Use Playwright (real browser) instead of requests when sites block or need JavaScript.
  # Enable if scraping fails with "No pages fetched" or connection/SSL errors.
  use_playwright: true
  
  # Delay between requests (in seconds) to respect rate limits
  delay_between_requests: 0.2
  
  # Maximum retry attempts for failed requests
  max_retries: 3
  
  # Base delay for exponential backoff on retries (in seconds)
  retry_delay_base: 1.0
  
  # Playwright wait strategy: "domcontentloaded" (fast), "load" (medium), "networkidle" (slow but best for dynamic content)
  playwright_wait_for: "domcontentloaded"

rag:
  # Size of text chunks for RAG indexing
  chunk_size: 800
  
  # Overlap between chunks
  chunk_overlap: 100

models:
  # Embedding model for vector search
  embed_model: "text-embedding-004"
  
  # Model for page categorization
  categorization_model: "gemini-2.5-flash"
